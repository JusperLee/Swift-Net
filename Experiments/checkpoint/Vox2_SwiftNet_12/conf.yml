audionet:
  audionet_config:
    audio_bn_params:
      is2d: true
      kernel_size: 1
      out_chan: 256
      pre_act_type: ReLU
      pre_norm_type: cLNhw
    audio_params:
      act_type: PReLU
      causal: true
      hid_chan: 64
      is2d: true
      kernel_size: 5
      layers:
        layer_1:
          bidirectional: true
          dim: 4
          hid_chan: 32
          kernel_size: 8
          num_layers: 4
          rnn_type: GRNN
          stride: 1
        layer_2:
          bidirectional: false
          dim: 3
          hid_chan: 64
          kernel_size: 8
          num_layers: 4
          rnn_type: GRNN
          stride: 1
        layer_3:
          act_type: PReLU
          causal: true
          dim: 3
          hid_chan: 4
          n_freqs: 65
          n_head: 4
          norm_type: LayerNormalization4D
      norm_type: cLNhw
      stride: 2
      upsampling_depth: 2
    causal: true
    enc_dec_params:
      act_type: null
      bias: false
      causal: true
      hop_length: 128
      kernel_size: 3
      norm_type: null
      out_chan: 256
      stride: 1
      win: 256
    fusion_params:
      fusion_shared: true
      fusion_type: SAFFusion
      is2d: true
      kernel_size: 4
    mask_generation_params:
      RI_split: true
      is2d: true
      mask_act: ReLU
      mask_generator_type: MaskGenerator
    n_src: 1
    pretrained_vout_chan: 512
    video_bn_params:
      kernel_size: -1
    video_params:
      act_type: PReLU
      causal: true
      hid_chan: 64
      is2d: false
      kernel_size: 3
      layers:
        layer_1:
          causal: true
          dropout: 0.1
          kernel_size: 3
          n_head: 8
          norm_type: cLN
      norm_type: BatchNorm1d
      stride: 2
  audionet_name: SwiftNet
datamodule:
  data_config:
    batch_size: 3
    n_src: 1
    normalize_audio: false
    num_workers: 8
    persistent_workers: false
    pin_memory: true
    sample_rate: 16000
    segment: 2.0
    test_dir: DataPreProcess/Vox2/tt
    train_dir: DataPreProcess/Vox2/tr
    valid_dir: DataPreProcess/Vox2/cv
  data_name: AVSpeechDataModule
exp:
  exp_name: Vox2_SwiftNet_12
loss:
  train:
    config:
      pit_from: pw_mtx
      threshold_byloss: true
    loss_func: PITLossWrapper
    sdr_type: pairwise_neg_snr
  val:
    config:
      pit_from: pw_mtx
      threshold_byloss: false
    loss_func: PITLossWrapper
    sdr_type: pairwise_neg_sisdr
main_args:
  conf_dir: configs/vox2_SwiftNet_12.yml
  exp_dir: /gpfs-flash/hulab/likai/Swift-Net/Experiments/checkpoint/Vox2_SwiftNet_12
  help: null
optimizer:
  lr: 0.001
  optim_name: adamw
  weight_decay: 0.1
positional arguments: {}
scheduler:
  sche_config:
    factor: 0.5
    patience: 10
  sche_name: ReduceLROnPlateau
training:
  early_stop:
    mode: min
    monitor: val_loss/dataloader_idx_0
    patience: 15
    verbose: true
  epochs: 500
  gpus:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  parallel: ddp
  system: AudioVisualLightningModule
videonet:
  videonet_config:
    backbone_type: resnet
    pretrain: pretrain_zoo/frcnn_128_512.backbone.pth.tar
    relu_type: prelu
    width_mult: 1.0
  videonet_name: FRCNNVideoModel
